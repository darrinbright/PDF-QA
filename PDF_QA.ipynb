{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 langchain langchain_community langchain_google_genai google-generativeai transformers faiss-gpu"
      ],
      "metadata": {
        "id": "b37aONFEpmHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MyY75tZWoKC2"
      },
      "outputs": [],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = '/content/Darrin_CV.pdf'\n",
        "text = \"\"\n",
        "with open(pdf_path, \"rb\") as file:\n",
        "    pdf_reader = PdfReader(file)\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text()"
      ],
      "metadata": {
        "id": "tDMFYw3cpM53"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "text_chunks = text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "9Vszzuc6pOxa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "google_api_key = 'AIzaSyARn_PcqweM5MXHxYaIWGQcf-BDJMP1bDw'\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=google_api_key)\n",
        "vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "vector_store.save_local(\"faiss_index\")"
      ],
      "metadata": {
        "id": "VOpAr6TWpQqd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'facebook/bart-large-cnn'\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "FSVnOwC_pStC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "Answer the question in a detailed way and include all the related details, if the answer is not in\n",
        "provided context just say, \"Answer is not available\", don't generate random responses\\n\\n\n",
        "Context:\\n {context}\\n\n",
        "Question: \\n{question}\\n\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.4, google_api_key=google_api_key)\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)"
      ],
      "metadata": {
        "id": "afJ47a9-pUxY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'Where is he doing his studies?'\n",
        "db = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "docs = db.similarity_search(question)\n",
        "\n",
        "if not docs:\n",
        "    print(\"No relevant documents found.\")\n",
        "else:\n",
        "    response = chain.invoke({\"input_documents\": docs, \"question\": question})\n",
        "    answer = response.get(\"output_text\", \"No answer generated.\")\n",
        "\n",
        "    if \"answer is not available in the context\" in answer.lower():\n",
        "        concatenated_text = \" \".join([doc.page_content for doc in docs])\n",
        "        inputs = tokenizer(concatenated_text, max_length=1024, return_tensors='pt', truncation=True)\n",
        "        summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=150, early_stopping=True)\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        answer += \"\\n\\nGenerated Summary/Insight: \" + summary\n",
        "\n",
        "    print(\"Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg4h-BilpWpu",
        "outputId": "f4abe848-c510-459e-94cc-ebd1464930bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Vellore Institute of Technology (VIT) Vellore, India\n"
          ]
        }
      ]
    }
  ]
}